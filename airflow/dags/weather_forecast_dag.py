import os
import csv
import json
import logging
import requests
import psycopg2
from datetime import datetime, date, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
from dotenv import load_dotenv

load_dotenv()

# C·∫•u h√¨nh logging
logger = logging.getLogger(__name__)

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,  # TƒÉng s·ªë l·∫ßn retry
    'retry_delay': timedelta(minutes=5),
}

provinces = [
    {"name": "An Giang", "lat": 10.521, "lon": 105.125},
    {"name": "B√† R·ªãa - V≈©ng T√†u", "lat": 10.541, "lon": 107.242},
    {"name": "B·∫°c Li√™u", "lat": 9.294, "lon": 105.724},
    {"name": "B·∫Øc Giang", "lat": 21.273, "lon": 106.194},
    {"name": "B·∫Øc K·∫°n", "lat": 22.147, "lon": 105.834},
    {"name": "B·∫Øc Ninh", "lat": 21.186, "lon": 106.076},
    {"name": "B·∫øn Tre", "lat": 10.243, "lon": 106.375},
    {"name": "B√¨nh D∆∞∆°ng", "lat": 11.325, "lon": 106.477},
    {"name": "B√¨nh ƒê·ªãnh", "lat": 14.166, "lon": 108.902},
    {"name": "B√¨nh Ph∆∞·ªõc", "lat": 11.751, "lon": 106.723},
    {"name": "B√¨nh Thu·∫≠n", "lat": 11.090, "lon": 108.072},
    {"name": "C√† Mau", "lat": 9.176, "lon": 105.152},
    {"name": "Cao B·∫±ng", "lat": 22.665, "lon": 106.257},
    {"name": "C·∫ßn Th∆°", "lat": 10.045, "lon": 105.746},
    {"name": "ƒê√† N·∫µng", "lat": 16.047, "lon": 108.206},
    {"name": "ƒê·∫Øk L·∫Øk", "lat": 12.710, "lon": 108.237},
    {"name": "ƒê·∫Øk N√¥ng", "lat": 12.264, "lon": 107.609},
    {"name": "ƒêi·ªán Bi√™n", "lat": 21.383, "lon": 103.016},
    {"name": "ƒê·ªìng Nai", "lat": 10.948, "lon": 106.824},
    {"name": "ƒê·ªìng Th√°p", "lat": 10.535, "lon": 105.636},
    {"name": "Gia Lai", "lat": 13.807, "lon": 108.109},
    {"name": "H√† Giang", "lat": 22.750, "lon": 104.983},
    {"name": "H√† Nam", "lat": 20.583, "lon": 105.922},
    {"name": "H√† N·ªôi", "lat": 21.028, "lon": 105.854},
    {"name": "H√† Tƒ©nh", "lat": 18.342, "lon": 105.905},
    {"name": "H·∫£i D∆∞∆°ng", "lat": 20.938, "lon": 106.330},
    {"name": "H·∫£i Ph√≤ng", "lat": 20.844, "lon": 106.688},
    {"name": "H·∫≠u Giang", "lat": 9.757, "lon": 105.641},
    {"name": "H√≤a B√¨nh", "lat": 20.817, "lon": 105.337},
    {"name": "H∆∞ng Y√™n", "lat": 20.646, "lon": 106.051},
    {"name": "Kh√°nh H√≤a", "lat": 12.259, "lon": 109.196},
    {"name": "Ki√™n Giang", "lat": 10.012, "lon": 105.080},
    {"name": "Kon Tum", "lat": 14.349, "lon": 108.000},
    {"name": "Lai Ch√¢u", "lat": 22.396, "lon": 103.458},
    {"name": "L√¢m ƒê·ªìng", "lat": 11.575, "lon": 108.142},
    {"name": "L·∫°ng S∆°n", "lat": 21.853, "lon": 106.761},
    {"name": "L√†o Cai", "lat": 22.485, "lon": 103.970},
    {"name": "Long An", "lat": 10.543, "lon": 106.411},
    {"name": "Nam ƒê·ªãnh", "lat": 20.438, "lon": 106.162},
    {"name": "Ngh·ªá An", "lat": 19.234, "lon": 104.920},
    {"name": "Ninh B√¨nh", "lat": 20.250, "lon": 105.974},
    {"name": "Ninh Thu·∫≠n", "lat": 11.564, "lon": 108.988},
    {"name": "Ph√∫ Th·ªç", "lat": 21.345, "lon": 105.254},
    {"name": "Ph√∫ Y√™n", "lat": 13.088, "lon": 109.092},
    {"name": "Qu·∫£ng B√¨nh", "lat": 17.468, "lon": 106.622},
    {"name": "Qu·∫£ng Nam", "lat": 15.539, "lon": 108.019},
    {"name": "Qu·∫£ng Ng√£i", "lat": 15.120, "lon": 108.800},
    {"name": "Qu·∫£ng Ninh", "lat": 21.006, "lon": 107.292},
    {"name": "Qu·∫£ng Tr·ªã", "lat": 16.744, "lon": 107.189},
    {"name": "S√≥c TrƒÉng", "lat": 9.602, "lon": 105.973},
    {"name": "S∆°n La", "lat": 21.325, "lon": 103.918},
    {"name": "T√¢y Ninh", "lat": 11.365, "lon": 106.098},
    {"name": "Th√°i B√¨nh", "lat": 20.446, "lon": 106.342},
    {"name": "Th√°i Nguy√™n", "lat": 21.594, "lon": 105.848},
    {"name": "Thanh H√≥a", "lat": 19.807, "lon": 105.776},
    {"name": "Th·ª´a Thi√™n Hu·∫ø", "lat": 16.463, "lon": 107.590},
    {"name": "Ti·ªÅn Giang", "lat": 10.449, "lon": 106.342},
    {"name": "TP H·ªì Ch√≠ Minh", "lat": 10.776, "lon": 106.700},
    {"name": "Tr√† Vinh", "lat": 9.812, "lon": 106.299},
    {"name": "Tuy√™n Quang", "lat": 21.823, "lon": 105.218},
    {"name": "Vƒ©nh Long", "lat": 10.253, "lon": 105.973},
    {"name": "Vƒ©nh Ph√∫c", "lat": 21.308, "lon": 105.604},
    {"name": "Y√™n B√°i", "lat": 21.705, "lon": 104.870}
]

def process_province_data(province, today_iso, end_date_iso):
    """
    X·ª≠ l√Ω d·ªØ li·ªáu th·ªùi ti·∫øt cho m·ªôt t·ªânh.
    ƒê∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ ch·∫°y song song v·ªõi ThreadPoolExecutor.
    """
    url = "https://api.open-meteo.com/v1/forecast"
    params = {
        "latitude": province["lat"],
        "longitude": province["lon"],
        "hourly": "temperature_2m,precipitation,windspeed_10m,uv_index,weathercode,sunshine_duration,relativehumidity_2m,apparent_temperature",
        "timezone": "Asia/Bangkok",
        "start_date": today_iso,
        "end_date": end_date_iso
    }

    province_results = []
    
    try:
        response = requests.get(url, params=params, timeout=10)  # Th√™m timeout ƒë·ªÉ tr√°nh treo
        if response.status_code == 200:
            data = response.json().get("hourly", {})
            times = data.get("time", [])
            
            # Ki·ªÉm tra n·∫øu kh√¥ng c√≥ d·ªØ li·ªáu time th√¨ b·ªè qua t·ªânh n√†y
            if not times:
                logger.warning(f"‚ö†Ô∏è {province['name']}: kh√¥ng c√≥ d·ªØ li·ªáu 'time' t·ª´ API, b·ªè qua.")
                return province_results

            # L·∫•y c√°c d·ªØ li·ªáu th·ªùi ti·∫øt
            temps = data.get("temperature_2m", [])
            precs = data.get("precipitation", [])
            winds = data.get("windspeed_10m", [])
            uvs = data.get("uv_index", [])
            weathercodes = data.get("weathercode", [])
            sunshines = data.get("sunshine_duration", [0]*len(times))
            humidity = data.get("relativehumidity_2m", [])
            feel_like = data.get("apparent_temperature", [])

            # X√¢y d·ª±ng ch·ªâ m·ª•c theo ng√†y
            day_indices = {}
            for idx, t in enumerate(times):
                day = t.split("T")[0]
                if day not in day_indices:
                    day_indices[day] = []
                day_indices[day].append(idx)

            # X·ª≠ l√Ω d·ªØ li·ªáu m·ªói 3 gi·ªù
            for i, t in enumerate(times):
                hour = int(t.split("T")[1][:2])
                if hour % 3 != 0:
                    continue
                
                day = t.split("T")[0]
                indices = day_indices.get(day, [])
                if not indices:
                    continue
                
                # T√≠nh to√°n c√°c gi√° tr·ªã th·ªëng k√™ theo ng√†y
                valid_temp_indices = [j for j in indices if j < len(temps)]
                valid_wind_indices = [j for j in indices if j < len(winds)]
                valid_uv_indices = [j for j in indices if j < len(uvs)]
                
                temp_max = max([temps[j] for j in valid_temp_indices]) if valid_temp_indices else None
                temp_min = min([temps[j] for j in valid_temp_indices]) if valid_temp_indices else None
                wind_max = max([winds[j] for j in valid_wind_indices]) if valid_wind_indices else None
                uv_max = max([uvs[j] for j in valid_uv_indices]) if valid_uv_indices else None
                
                # T√≠nh s·ªë gi·ªù n·∫Øng v√† ƒë√™m
                valid_sunshine_indices = [j for j in indices if j < len(sunshines) and 6 <= int(times[j].split("T")[1][:2]) < 18]
                sunshine_hours = sum([sunshines[j] for j in valid_sunshine_indices]) / 3600 if valid_sunshine_indices else 0
                
                valid_time_indices = [j for j in indices if j < len(times)]
                sundown_hours = len([j for j in valid_time_indices if int(times[j].split("T")[1][:2]) < 6 or int(times[j].split("T")[1][:2]) >= 18])
                
                # ƒê·∫£m b·∫£o index i kh√¥ng v∆∞·ª£t qu√° ƒë·ªô d√†i c·ªßa c√°c list
                current_temp = temps[i] if i < len(temps) else None
                current_prec = precs[i] if i < len(precs) else None
                current_weathercode = weathercodes[i] if i < len(weathercodes) else None
                current_humidity = humidity[i] if i < len(humidity) else None
                current_feel_like = feel_like[i] if i < len(feel_like) else None

                # Th√™m v√†o danh s√°ch k·∫øt qu·∫£
                province_results.append([
                    province["name"],
                    t,
                    current_temp,
                    temp_max,
                    temp_min,
                    current_prec,
                    wind_max,
                    uv_max,
                    round(sunshine_hours, 2),
                    sundown_hours,
                    current_weathercode,
                    current_humidity,
                    current_feel_like
                ])
            
            logger.info(f"‚úÖ {province['name']}: xong - {len(province_results)} b·∫£n ghi")
            return province_results
        else:
            logger.error(f"‚ùå {province['name']}: l·ªói API - Status {response.status_code}")
            return []
    except Exception as e:
        logger.error(f"‚ùå {province['name']}: l·ªói x·ª≠ l√Ω - {str(e)}")
        return []

def fetch_weather_data(**context):
    today = date.today()
    today_iso = today.isoformat()
    end_date = today + timedelta(days=2)
    end_date_iso = end_date.isoformat()
    
    # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
    data_dir = "/opt/airflow/weather_data"
    os.makedirs(data_dir, exist_ok=True)
    
    csv_file = f"{data_dir}/{today_iso}_{end_date_iso}.csv"
    header = [
        "Province", "Time", "Temperature", "Temp_Max", "Temp_Min", 
        "Precipitation", "Windspeed_Max", "UV_Index_Max", "Sunshine_Hours", 
        "Sundown_Hours", "Weather_Code", "Humidity", "Feel_Like"
    ]
    
    logger.info(f"üîÑ B·∫Øt ƒë·∫ßu thu th·∫≠p d·ªØ li·ªáu th·ªùi ti·∫øt cho {len(provinces)} t·ªânh/th√†nh t·ª´ {today_iso} ƒë·∫øn {end_date_iso}")
    
    # S·ª≠ d·ª•ng ThreadPoolExecutor ƒë·ªÉ thu th·∫≠p d·ªØ li·ªáu song song
    all_results = []
    max_workers = min(10, len(provinces))  # T·ªëi ƒëa 10 threads ƒë·ªÉ tr√°nh qu√° t·∫£i
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # T·∫°o c√°c future tasks
        future_to_province = {
            executor.submit(process_province_data, province, today_iso, end_date_iso): province["name"]
            for province in provinces
        }
        
        # L·∫•y k·∫øt qu·∫£ khi c√°c task ho√†n th√†nh
        for future in as_completed(future_to_province):
            province_name = future_to_province[future]
            try:
                province_results = future.result()
                all_results.extend(province_results)
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi x·ª≠ l√Ω d·ªØ li·ªáu c·ªßa {province_name}: {str(e)}")
    
    # Ghi d·ªØ li·ªáu v√†o file CSV
    with open(csv_file, mode="w", newline='', encoding="utf-8-sig") as f:
        writer = csv.writer(f)
        writer.writerow(header)
        writer.writerows(all_results)
    
    # Th·ªëng k√™ k·∫øt qu·∫£
    province_counts = {}
    for row in all_results:
        province_name = row[0]
        if province_name in province_counts:
            province_counts[province_name] += 1
        else:
            province_counts[province_name] = 1
    
    # Hi·ªÉn th·ªã th·ªëng k√™
    logger.info(f"\nüìä TH·ªêNG K√ä K·∫æT QU·∫¢:")
    logger.info(f"‚úÖ T·ªïng s·ªë b·∫£n ghi: {len(all_results)}")
    logger.info(f"‚úÖ S·ªë t·ªânh/th√†nh c√≥ d·ªØ li·ªáu: {len(province_counts)}")
    logger.info(f"üìÑ ƒê√£ l∆∞u v√†o file: {csv_file}")
    
    return csv_file

def import_to_database(**context):
    ti = context['ti']
    csv_file_path = ti.xcom_pull(task_ids='fetch_weather_data')
    
    if not csv_file_path or not os.path.exists(csv_file_path):
        logger.error(f"‚ùå Kh√¥ng t√¨m th·∫•y file CSV: {csv_file_path}. B·ªè qua import.")
        return False

    # X√°c ƒë·ªãnh kho·∫£ng th·ªùi gian d·ªØ li·ªáu
    start_date_for_delete = date.today()
    end_date_for_delete = start_date_for_delete + timedelta(days=2)
    
    connection = None
    try:
        # K·∫øt n·ªëi ƒë·∫øn database
        db_host = os.environ.get("DATABASE_HOST", "host.docker.internal")
        db_config = {
            "dbname": os.environ.get("DATABASE_NAME"),
            "user": os.environ.get("DATABASE_USER"),
            "password": os.environ.get("DATABASE_PASSWORD"),
            "host": db_host,
            "port": os.environ.get("DATABASE_PORT")
        }
        
        connection = psycopg2.connect(**db_config)
        cursor = connection.cursor()
        table_name = os.environ.get("WEATHER_DATA_TABLE_NAME", "weather_data")

        logger.info(f"üîÑ B·∫Øt ƒë·∫ßu nh·∫≠p d·ªØ li·ªáu t·ª´ {csv_file_path} v√†o b·∫£ng {table_name}...")
        
        # X√≥a d·ªØ li·ªáu c≈© trong kho·∫£ng th·ªùi gian
        delete_start_timestamp = datetime.combine(start_date_for_delete, datetime.min.time())
        delete_end_timestamp = datetime.combine(end_date_for_delete, datetime.max.time())

        logger.info(f"üóëÔ∏è X√≥a d·ªØ li·ªáu c≈© trong kho·∫£ng: {delete_start_timestamp} ƒë·∫øn {delete_end_timestamp}")
        cursor.execute(
            f"DELETE FROM {table_name} WHERE time >= %s AND time <= %s",
            (delete_start_timestamp, delete_end_timestamp)
        )
        
        # ƒê·∫øm s·ªë b·∫£n ghi tr∆∞·ªõc khi import
        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
        records_before = cursor.fetchone()[0]
        
        # Import d·ªØ li·ªáu m·ªõi b·∫±ng COPY
        with open(csv_file_path, mode="r", encoding="utf-8-sig") as file_obj:
            next(file_obj)  # B·ªè qua header
            copy_sql = f"""
                COPY {table_name} (
                    province, time, temperature, temp_max, temp_min, precipitation,
                    windspeed_max, uv_index_max, sunshine_hours, sundown_hours,
                    weather_code, humidity, feel_like
                )
                FROM STDIN
                WITH (FORMAT CSV, HEADER FALSE);
            """
            cursor.copy_expert(sql=copy_sql, file=file_obj)
        
        # ƒê·∫øm s·ªë b·∫£n ghi sau khi import
        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
        records_after = cursor.fetchone()[0]
        records_added = records_after - records_before
        
        connection.commit()
        logger.info(f"‚úÖ Import th√†nh c√¥ng! ƒê√£ th√™m {records_added} b·∫£n ghi v√†o database.")
        
        # L∆∞u th√¥ng tin ƒë·ªÉ task ki·ªÉm tra c√≥ th·ªÉ s·ª≠ d·ª•ng
        context['ti'].xcom_push(key='import_success', value=True)
        context['ti'].xcom_push(key='date_range', value={
            'start_date': delete_start_timestamp.isoformat(),
            'end_date': delete_end_timestamp.isoformat(),
            'table_name': table_name,
            'records_added': records_added
        })
        
        return True

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi nh·∫≠p d·ªØ li·ªáu v√†o database: {str(e)}")
        if connection:
            connection.rollback()
        context['ti'].xcom_push(key='import_success', value=False)
        context['ti'].xcom_push(key='import_error', value=str(e))
        return False
        
    finally:
        if connection:
            cursor.close()
            connection.close()
            logger.info("‚ÑπÔ∏è ƒê√£ ƒë√≥ng k·∫øt n·ªëi t·ªõi PostgreSQL.")

def verify_import(**context):
    ti = context['ti']
    import_success = ti.xcom_pull(key='import_success', task_ids='import_to_database')
    date_range = ti.xcom_pull(key='date_range', task_ids='import_to_database')
    
    if not import_success:
        error_msg = ti.xcom_pull(key='import_error', task_ids='import_to_database') or "L·ªói kh√¥ng x√°c ƒë·ªãnh"
        logger.error(f"‚ùå Import d·ªØ li·ªáu th·∫•t b·∫°i: {error_msg}")
        return False
    
    connection = None
    try:
        # K·∫øt n·ªëi database ƒë·ªÉ ki·ªÉm tra
        db_host = os.environ.get("DATABASE_HOST", "host.docker.internal")
        db_config = {
            "dbname": os.environ.get("DATABASE_NAME"),
            "user": os.environ.get("DATABASE_USER"),
            "password": os.environ.get("DATABASE_PASSWORD"),
            "host": db_host,
            "port": os.environ.get("DATABASE_PORT")
        }
        
        connection = psycopg2.connect(**db_config)
        cursor = connection.cursor()
        
        table_name = date_range['table_name']
        start_date = date_range['start_date']
        end_date = date_range['end_date']
        
        # Ki·ªÉm tra t·ªïng s·ªë b·∫£n ghi v√† s·ªë t·ªânh/th√†nh
        cursor.execute(
            f"""
            SELECT COUNT(*), COUNT(DISTINCT province)
            FROM {table_name}
            WHERE time >= %s AND time <= %s
            """,
            (start_date, end_date)
        )
        total_records, province_count = cursor.fetchone()
        
        # Ki·ªÉm tra ph·∫°m vi th·ªùi gian
        cursor.execute(
            f"""
            SELECT MIN(time), MAX(time)
            FROM {table_name}
            WHERE time >= %s AND time <= %s
            """,
            (start_date, end_date)
        )
        earliest, latest = cursor.fetchone()
        
        # Th·ªëng k√™ theo t·ªânh/th√†nh
        cursor.execute(
            f"""
            SELECT province, COUNT(*)
            FROM {table_name}
            WHERE time >= %s AND time <= %s
            GROUP BY province
            ORDER BY COUNT(*) DESC
            """,
            (start_date, end_date)
        )
        province_stats = cursor.fetchall()
        
        # Hi·ªÉn th·ªã k·∫øt qu·∫£ ki·ªÉm tra
        logger.info("\n====== üìä K·∫æT QU·∫¢ KI·ªÇM TRA D·ªÆ LI·ªÜU ======")
        logger.info(f"‚úÖ T·ªïng s·ªë b·∫£n ghi: {total_records}")
        logger.info(f"‚úÖ S·ªë t·ªânh/th√†nh: {province_count}")
        
        if earliest and latest:
            logger.info(f"‚úÖ Th·ªùi gian b·∫£n ghi ƒë·∫ßu ti√™n: {earliest}")
            logger.info(f"‚úÖ Th·ªùi gian b·∫£n ghi cu·ªëi c√πng: {latest}")
            logger.info(f"‚úÖ T·ªïng th·ªùi gian d·ªØ li·ªáu: {(latest - earliest).days} ng√†y, {(latest - earliest).seconds // 3600} gi·ªù")
        
        # Hi·ªÉn th·ªã th·ªëng k√™ theo t·ªânh
        logger.info("\n------ Chi ti·∫øt theo t·ªânh/th√†nh ------")
        for i, (province, count) in enumerate(province_stats[:10]):
            logger.info(f"{i+1}. {province}: {count} b·∫£n ghi")
        
        if len(province_stats) > 10:
            logger.info(f"... v√† {len(province_stats) - 10} t·ªânh/th√†nh kh√°c")
        
        # L∆∞u b√°o c√°o chi ti·∫øt
        report = {
            'total_records': total_records,
            'province_count': province_count,
            'earliest': earliest.isoformat() if earliest else None,
            'latest': latest.isoformat() if latest else None,
            'provinces': [{'name': p, 'record_count': c} for p, c in province_stats],
            'import_time': datetime.now().isoformat(),
            'date_range': {
                'start': start_date,
                'end': end_date
            }
        }
        
        # L∆∞u b√°o c√°o v√†o file
        report_file = os.path.join(
            os.path.dirname(context['ti'].xcom_pull(task_ids='fetch_weather_data')),
            f"import_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"\nüìÑ ƒê√£ l∆∞u b√°o c√°o chi ti·∫øt v√†o file: {report_file}")
        
        # X√°c nh·∫≠n th√†nh c√¥ng
        expected_provinces = len(provinces)
        if total_records > 0 and province_count >= expected_provinces * 0.9:  # N·∫øu c√≥ √≠t nh·∫•t 90% t·ªânh c√≥ d·ªØ li·ªáu
            logger.info("\n‚úÖ TH√ÄNH C√îNG: D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c import ƒë·∫ßy ƒë·ªß v√†o database!")
            return True
        elif total_records > 0:
            logger.warning(f"\n‚ö†Ô∏è CH√ö √ù: D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c import nh∆∞ng ch·ªâ c√≥ {province_count}/{expected_provinces} t·ªânh/th√†nh!")
            return True
        else:
            logger.error("\n‚ùå TH·∫§T B·∫†I: Kh√¥ng c√≥ b·∫£n ghi n√†o ƒë∆∞·ª£c import v√†o database!")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi ki·ªÉm tra d·ªØ li·ªáu: {str(e)}")
        return False
    finally:
        if connection:
            cursor.close()
            connection.close()
            logger.info("‚ÑπÔ∏è ƒê√£ ƒë√≥ng k·∫øt n·ªëi t·ªõi PostgreSQL.")

with DAG(
    dag_id='weather_forecast_dag', 
    default_args=default_args,
    description='Thu th·∫≠p v√† l∆∞u tr·ªØ d·ªØ li·ªáu d·ª± b√°o th·ªùi ti·∫øt cho 63 t·ªânh th√†nh Vi·ªát Nam',
    schedule_interval=timedelta(days=1),  # C·∫≠p nh·∫≠t h√†ng ng√†y thay v√¨ 3 ng√†y
    start_date=days_ago(1),
    catchup=False,
    tags=['weather', 'forecast', 'vietnam'],
) as dag:

    fetch_task = PythonOperator(
        task_id='fetch_weather_data',
        python_callable=fetch_weather_data,
    )

    import_task = PythonOperator(
        task_id='import_to_database',
        python_callable=import_to_database,
    )
    
    verify_task = PythonOperator(
        task_id='verify_import',
        python_callable=verify_import,
    )

    # Thi·∫øt l·∫≠p ph·ª• thu·ªôc gi·ªØa c√°c task
    fetch_task >> import_task >> verify_task